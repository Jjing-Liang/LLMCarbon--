# Title

## Introduction

Since the introduction of ChatGPT, the Large Language Model (LLM) has evolved rapidly - GPT-4, LLama, Claude, Gemini, Mistral, Grok-1 - one new model after another is influencing the whole society. At the same time, the impact of LLM on the environment has also been hotly debated. At this stage, there are many studies or articles discussing the impact of AI/ML/LLM on the environment. A study at the University of Washington (https://www.washington.edu/news/2023/07/27/how-much-energy-does-chatgpt-use/) shows that training just one chatbot can consume a year's worth of electricity in a neighborhood. The actual carbon emissions generated behind the scenes are still not transparent today.

We describe the process of calculating carbon emissions from the perspective of the LLM's lifecycle in the two main phases of carbon generation: training and inference, while using the [Impact framework](https://if.greensoftware.foundation/) tool by [Green Software Foundation](https://greensoftware.foundation/) to calculate the carbon emissions of a chatbot. We created a demo of the LLM life cycle carbon emissions for your reference. In the process of gathering information, we have compiled an up-to-date dataset containing the carbon emissions of existing LLMs, and a summary of publicly available data related to carbon emissions calculations (Carbon Dioxide Equivalent, PUE, Carbon intensity, etc.). We hope that the key information presented in this blog will help you understand the carbon footprint of LLMs from an environmentally responsible perspective, so that you can train and use LLMs wisely.


## Methodology for estimating LLM carbon emissions

<<<<<<< HEAD
## Basic of Energy and CO2e

- Carbon Dioxide Equivalent (CO2e)

    Carbon Dioxide Equivalent, or CO2e, is a measurement term used to describe different greenhouse gases in a common unit. CO2e is also often written as CO2eq, CO2-eq or CO2equivalent.

- Carbon Intensity

    Carbon intensity is measured in grams of carbon dioxide equivalents (CO2e) emitted per kilowatt-hour (KWh) of electricity generated. The standard unit of carbon intensity is gCO2eq/kWh.

- Embodied Carbon

    Embodied carbon (also referred to as "embedded carbon") is the amount of carbon pollution emitted during the creation and disposal of a device.

- Power usage effectiveness (PUE)

    Power usage effectiveness (PUE) is a ratio to measure data center energy efficiency.

- Thermal Design Power (TDP)

    The thermal design power (TDP) is the maximum amount of heat generated by a computer chip or component (often a CPU, GPU or system on a chip) that the cooling system in a computer is designed to dissipate under any workload. It will be used to estimate energy comsumption for GPUs.

- Carbon emitted Per unit Area (CPA)

    Carbon emitted Per unit Area (CPA) is used to quantify the embedded carbon of a chip, which depends on various semiconductor manufacturing parameters, including yield, energy consumption per unit area during the manufacturing process, emissions from chemicals used in hardware production, and emissions related to raw material procurement. The specific calculation formula is derived from [Faiz et al., 2023](https://arxiv.org/abs/2309.14393)

## Training CO2e calculation
=======
### Training CO2e calculation
>>>>>>> f593ab4 ([yuyan] add manifest)

The total carbon footprint ``CO2eq`` resulting from LLM processing is determined by
```
CO2eq = CO2eq_oper + CO2eq_emb
```
where ``CO2eq_oper`` indicates the operational carbon footprint of the LLM,
and ``CO2eq_emb`` denotes the embodied carbon footprint of the LLM.

### Embodied carbon footprint
To quantify the chip‚Äôs embodied carbon footprint ``CO2eq_chip`` within a specific hardware unit is calculated by
```
CO2eq_chip = area * CPA
```
where ``area`` represents the chip‚Äôs area, ``CPA`` means the carbon emitted per unit area.

### Operational carbon footprint
The operational carbon footprint ``CO2eq_oper`` attributed to LLM processing is calculated by
```
CO2eq_oper = energy_oper * carb_inten
```
where ``energy_oper`` represents the operational energy for LLM processing, and ``carb_inten`` denotes
the carbon intensity of the specific data center.

#### Operational energy
The operational energy ``energy_oper`` associated with LLM processing can be calculated by
```
energy_oper = energy_hard * PUE
```
where ``energy_hard`` denotes the energy used by the computing hardware within a data center, and
``PUE`` indicates the PUE of the specific data center.

#### Hardware energy
The single unit ``i`` energy ``energy_hard_i`` consumed by
```
energy_hard_i = P_i * eff_i * n_i * t_i
```
where  
``P_i`` denotes the peak power of hardware unit ``i``;  
``eff_i`` represents the hardware
efficiency of hardware unit ``i``;  
``n_i`` indicates the count of hardware unit ``i``;  
``t_i``means the execution time of hardware unit ``i``;  
Hardware units encompass a range of components, including CPUs, LLM computing devices, memories, SSDs, and others.
The total energy ``energy_hard`` consumed by all hardware units.

#### Training time
To calculate hardware energy, it is necessary to have the training time. The training time can be estimated by the following:
- Total train FLOPs required by the model
- Benchmark of single GPU FLOPs
- Percent of peak device throughput as estimated using the regression equation
```
T = C / (n * FLOP_peak * eff)
```
where ``C`` represents the computation required to train the transformer model, in total floating point operations, ``FLOP_peak`` represents the device peak throughput, ``eff`` represents efficiency of the device.


###3 Hardware efficiency
Efficient processing of LLMs relies on achieving high hardware efficiency, which is calculated as the actual computing throughput divided by the peak throughput.
The actual computing throughput is calculated as total floating point operations divided by execution time.

##### Hardware efficiency estimation
If training time is not recorded, throughput is estimated to find total train time and carbon emission. A linear regression using a 2nd order polynomial is fit on the throughput scaling data presented in the paper [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473).
The optimal parallelism setting is represented as ``p``,``t``,``d``,``e``, where each variable corresponds to a degree of pipeline, tensor, data, and expert parallelism, respectively.
The efficiency ``eff_re`` with ``re`` devices can be calculated by
```
when re < n,
eff_re = (r_0 * re) / (n * eff_n)

when re > n,
eff_re = (r_1 * re) / (n * eff_n) + r_2 * re
```
, where ``r_0``, ``r_1``, ``r_2``are fitting constants, ``eff_n``means the highest hardware efficiency,
and ``n``indicates the number of devices that can achieve ``eff_n``. The number of devices required to achieve optimal hardware efficiency for dense LLM processing is calculated as
``n = t ‚ãÖ p ‚ãÖ d``.

#### Floating point operations
With ``l`` transformer layers, hidden size ``h``, sequence length ``s``, vocabulary size ``V``, and training batch size ``B``,
a transformer layer consists of an attention block followed by a 2-layer feed-forward network. A ùê¥ùëö√óùëò √ó ùëãùëò√óùëõ matrix multiplication requires 2ùëö √ó ùëò √ó ùëõ FLOPs (factor of 2 needed to account for multiplies and adds).

For the attention block, the main FLOP contributors are the key, query, and value transformation (``6Bsh^2`` operations), 
attention matrix computation (``2Bs^2h`` operations), attention over values (``2Bs^2h`` operations), 
and post-attention linear projection (``2Bsh^2`` operations). The feed-forward network increases the hidden size to ``4h`` and then reduces it back to ``h``; this requires ``16Bsh^2`` FLOPs. 
Summing these together, each transformer layer results in ``24Bsh^2 + 4Bs^2h`` FLOPs for the forward pass.

The other main contributor to the FLOP count is the logit layer in the language model head, the required FLOPs for this operation is ``2Bshùëâ`` in the forward pass and ``4Bshùëâ`` in the backward pass, resulting in ``6Bshùëâ`` FLOPs in total.

The backward pass requires double the number of FLOPs since need to calculate the gradients with respect to both input and weight tensors.

Thus, for a transformer model with ``l`` transformer layers, the total number of floating-point operations is:
```
C = C_forward + C_backward ‚âà 2PD + 4PD ‚âà 6PD
```
with parameter size ``P`` and the training dataset size ``D`` (tokens).

#### Parameters size
The number of parameters in a model ``P`` can be computed as:
```
P = 12lh^2 * [1 + 13/12h + (V + s)/(12lh)]
```
where number of layers ``l``, hidden size ``h``, vocabulary size ``V``, and sequence length ``s``.


### Inference CO2e calculation
The total carbon footprint calculation of inference is similar to training. Inference involves running the input data through the model's forward pass without performing 
any backward pass or gradient updates, thus the computation ``C_inference`` is approximated as
```
C_inference ‚âà 2P * D_inference
```
where ``D_inference`` means inference dataset size (tokens).


## Using Impact Framework for estimation

<<<<<<< HEAD
### What is Impact Framework

[Impact Framework (IF)](https://if.greensoftware.foundation/) is an Open Source tool being run inside the Green Software Foundation designed to assess the environmental impact of software across various components and settings, aiming to minimize the ecological footprint of software. To utilize IF, you simply need to create a manifest file, after which the IF takes care of the remaining processes. This manifest file provides essential context for calculating the environmental impact, outlining the application's architecture, the duration of observation, the sequence of calculations and transformations to be performed, and the specific environmental metrics to be monitored.

Here is the video explain how IF works, it can help you better understand the capabilities of IF.

[![impact framework explainer video)](https://github.com/Green-Software-Foundation/hack/assets/11027021/541b9e21-85ab-47ab-8d43-c8e2111408a7)](https://www.youtube.com/watch?v=msk-55owTeM)

## Conclusion

=======
With the methodology outlined above for estimating LLM carbon emissions information, we can utilize the Impact Framework to assess the carbon footprint of the LLM. The Impact Framework offers a versatile and expandable framework for evaluating the carbon footprint of diverse computing activities, leveraging a variety of plugins to build upon the manifest.

### Basic Manifest for LLM Carbon Emissions

Based on the basic the total carbon footprint equation `CO2eq = CO2eq_oper + CO2eq_emb`, we can divide the total carbon footprint into two components: `CO2eq_oper`,the operational footprint, and `CO2eq_emb`, the embodied footprint.

#### Basic Operational Footprint Equation and Variables
The fundamental equation for `CO2eq_oper` is `CO2eq_oper = energy_oper * carb_inten`, where `energy_oper` represents the energy utilized during the operation of the LLM, and `carb_inten` denotes the carbon intensity of the energy consumed. To derive `energy_oper`, the [Watt-hour formula](https://arxiv.org/abs/2111.00364) `energy_oper(Wh) = GPU-num√óGPU-h√óTDP√óPUE` is employed. Hence, acquiring `energy_oper` necessitates knowledge of the total hours for training an LLM (`GPU-num√óGPU-h`), the power consumption of the GPU (TDP), and the Power Usage Effectiveness (`PUE`).

The equivalent training carbon footprint depends on:
- Total Training Time
- Number of GPUs
- Thermal Design Power(TDP) of GPUs
- Power Usage Effectiveness(PUE)
- Regional carbon equivalent emissions


The final equation for operational footprint is:

```
CO2eq_oper =  GPU-num*GPU-h*(GPU power consumption per GPU) * PUE * carb_inten
```

#### Basic Embodied Footprint Equation and Variables

#### Dive in Manifest


### Extended Manifest for LLM Carbon Emissions

## Conclusion

## Reference
Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Glo- ria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. 2022. Sustainable ai: Environmental implica- tions, challenges and opportunities. Proceedings of Machine Learning and Systems, 4:795‚Äì813.
>>>>>>> f593ab4 ([yuyan] add manifest)
