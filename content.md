# Title

## Introduction

Since the introduction of ChatGPT, the Large Language Model (LLM) has evolved rapidly - GPT-4, LLama, Claude, Gemini, Mistral, Grok-1 - one new model after another is influencing the whole society. At the same time, the impact of LLM on the environment has also been hotly debated. At this stage, there are many studies or articles discussing the impact of AI/ML/LLM on the environment. [A study](https://www.washington.edu/news/2023/07/27/how-much-energy-does-chatgpt-use/) at the University of Washington shows that training just one chatbot can consume a year's worth of electricity in a neighborhood. The actual carbon emissions generated behind the scenes are still not transparent today.

We delve into the carbon footprint of Large Language Models (LLMs) across their critical lifecycle stages: training and inference. Utilizing the [Impact framework](https://if.greensoftware.foundation/) tool developed by the [Green Software Foundation](https://greensoftware.foundation/), we provide a detailed methodology for calculating the carbon emissions associated with LLM. We've crafted a series of manifest examples that illustrate the carbon emissions profile throughout the LLM's life cycle, offering a reference for your analysis. Throughout our research, we've organize a current dataset on the carbon emissions of various LLMs, alongside a digest of relevant public data on carbon emissions metrics, such as Carbon Dioxide Equivalent and Carbon Intensity. Our aim is to equip you with the essential knowledge to approach the training and utilization of LLMs with an environmentally conscious mindset, fostering wise decision-making in the realm of AI.

## Methodology for estimating LLM carbon emissions

### Basic of Energy and CO2e

- Carbon Dioxide Equivalent (CO2e)

  Carbon Dioxide Equivalent, or CO2e, is a measurement term used to describe different greenhouse gases in a common unit. CO2e is also often written as CO2eq, CO2-eq or CO2equivalent.

- Carbon Intensity

  Carbon intensity is measured in grams of carbon dioxide equivalents (CO2e) emitted per kilowatt-hour (KWh) of electricity generated. The standard unit of carbon intensity is gCO2eq/kWh. You can find our collected data source on [here](./data/impact.md).

- Embodied Carbon

  Embodied carbon (also referred to as "embedded carbon") is the amount of carbon pollution emitted during the creation and disposal of a device.

- Power usage effectiveness (PUE)

  Power usage effectiveness (PUE) is a ratio to measure data center energy efficiency. For example, when the PUE is 1.43, this means that for every 1.43 units of energy used to run the entire data center, only 1 unit is effectively used for computing, with the remaining 0.43 units being used for non-computing purposes, such as cooling and lighting. You can find our collected data source on [here](./data/impact.md).

- Thermal Design Power (TDP)

  The thermal design power (TDP) is the maximum amount of heat generated by a computer chip or component (often a CPU, GPU or system on a chip) that the cooling system in a computer is designed to dissipate under any workload. It will be used to estimate energy comsumption for GPUs. You can find our collected data source on [here](./data/GPUs.md).

- Carbon emitted Per unit Area (CPA)

  Carbon emitted Per unit Area (CPA) is used to quantify the embedded carbon of a chip, which depends on various semiconductor manufacturing parameters, including yield, energy consumption per unit area during the manufacturing process, emissions from chemicals used in hardware production, and emissions related to raw material procurement. The specific calculation formula is derived from [Faiz et al., 2023](https://arxiv.org/abs/2309.14393). You can find our collected data source on [here](./data/embodied-carbon-CPA.md).

### Training CO2e calculation
The total carbon footprint ``CO2eq`` resulting from LLM processing consists of two main components: the operational carbon footprint and the embodied carbon footprint.

The operational carbon footprint refers to the carbon emissions generated during the day-to-day operations of the LLM. This includes the energy consumption required for training, inference, and other computational processes. The carbon emissions are produced mainly through the use of electricity to power the hardware infrastructure and the associated cooling systems.

The embodied carbon footprint represents the carbon emissions associated with the manufacturing, transportation, and disposal of the physical infrastructure used to support the LLM. This includes the carbon emissions generated during the production of servers, storage devices, and networking equipment, as well as the embodied energy in the materials used.

The total carbon footprint ``CO2eq`` resulting from LLM processing is determined by
```
CO2eq = CO2eq_oper + CO2eq_emb
```
where ``CO2eq_oper`` indicates the operational carbon footprint of the LLM,
and ``CO2eq_emb`` denotes the embodied carbon footprint of the LLM.

#### Embodied carbon footprint
The total embodied carbon footprint ``CO2eq_emb`` originating from all hardware units involved in LLM processing,
when each unit ``i`` is assessed using the following
```
CO2eq_emb_i = (t_i * CO2eq_chip_i) / lifetime_i
```
where ``CO2eq_chip_i`` denotes the chip’s embodied carbon footprint for hardware unit ``i``, ``lifetime_i`` means the lifespan of hardware unit ``i``,
and ``t_i`` represents the execution duration of hardware unit ``i``.

To quantify the chip’s embodied carbon footprint ``CO2eq_chip`` within a specific hardware unit is calculated by
```
CO2eq_chip = area * CPA
```
where ``area`` represents the chip’s area, ``CPA`` means the carbon emitted per unit area.

#### Operational carbon footprint
The operational carbon footprint ``CO2eq_oper`` attributed to LLM processing is calculated by
```
CO2eq_oper = energy_oper * carb_inten
```
``energy_oper`` includes the energy used for training, inference, and other computational processes involved in running the LLM. It takes into account the power consumption of the hardware infrastructure, including servers, networking equipment, and cooling systems.

``carb_inten`` refers to the carbon intensity of the specific data center where the LLM processing takes place. Carbon intensity represents the amount of carbon emissions associated with the energy generation and consumption in the data center. It takes into account factors such as the energy sources used (e.g., coal, natural gas, renewable energy), the efficiency of the energy generation, and any carbon offset or reduction measures in place.

By multiplying the operational energy with the carbon intensity, we can estimate the carbon emissions or carbon dioxide equivalent attributed to the operational phase of LLM processing. This calculation helps quantify the environmental impact and carbon footprint associated with the energy consumption during the operation of the LLM.

#### Operational energy
By multiplying the energy consumption of the computing hardware with the PUE of the specific data center, we can estimate the total energy consumed during LLM processing. 
This calculation takes into account the energy requirements of the hardware as well as the efficiency of the data center's infrastructure in delivering that energy to the IT equipment.

The operational energy ``energy_oper`` associated with LLM processing can be calculated by
```
energy_oper = energy_hard * PUE
```
``energy_hard`` represents the energy consumed by the computing hardware within a data center. This includes the energy used by servers, storage devices, networking equipment, and other hardware components involved in LLM processing.

``PUE`` is a metric that quantifies the energy efficiency of a data center. It represents the ratio of the total energy consumed by the data center, including both IT equipment and supporting infrastructure (such as cooling systems and power distribution), to the energy consumed by the IT equipment alone.

#### Hardware energy
The total energy ``energy_hard`` consumed by all hardware units. The single unit ``i`` energy ``energy_hard_i`` consumed by
```
energy_hard_i = P_i * eff_i * n_i * t_i
```

where  
``P_i`` denotes the peak power of hardware unit ``i``;  
``eff_i`` represents the hardware
efficiency of hardware unit ``i``;  
``n_i`` indicates the count of hardware unit ``i``;  
``t_i``means the execution time of hardware unit ``i``;  
Hardware units encompass a range of components, including CPUs, LLM computing devices, memories, SSDs, and others.

Hardware efficiency and training time are related in the context of machine learning and deep learning tasks.  

#### Training time
The training time can be estimated by the following:
- Total train FLOPs required by the model
- Benchmark of single GPU FLOPs
- Percent of peak device throughput as estimated using the regression equation
```
T = C / (n * FLOP_peak * eff)
```
where ``C`` represents the computation required to train the transformer model, in total floating point operations, ``FLOP_peak`` represents the device peak throughput, ``eff`` represents efficiency of the device.

#### Hardware efficiency
Hardware efficiency refers to how effectively the hardware resources are utilized to perform computations during the training process. Efficient hardware design and architecture can lead to faster and more optimized computations, resulting in shorter training times.
It is calculated as the actual computing throughput divided by the peak throughput. The actual computing throughput is calculated as total floating point operations divided by execution time.

##### Hardware efficiency estimation
A linear regression using a 2nd order polynomial is fit on the throughput scaling data presented in the paper [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473).
The optimal parallelism setting is represented as ``p``,``t``,``d``,``e``, where each variable corresponds to a degree of pipeline, tensor, data, and expert parallelism, respectively.
The efficiency ``eff_re`` with ``re`` devices can be calculated by
```
when re < n,
eff_re = (r_0 * re) / (n * eff_n)

when re > n,
eff_re = (r_1 * re) / (n * eff_n) + r_2 * re
```
, where ``r_0``, ``r_1``, ``r_2``are fitting constants, ``eff_n``means the highest hardware efficiency,
and ``n``indicates the number of devices that can achieve ``eff_n``. The number of devices required to achieve optimal hardware efficiency for dense LLM processing is calculated as
``n = t ⋅ p ⋅ d``.

#### Floating point operations
With ``l`` transformer layers, hidden size ``h``, sequence length ``s``, vocabulary size ``V``, and training batch size ``B``,
a transformer layer consists of an attention block followed by a 2-layer feed-forward network. A 𝐴𝑚×𝑘 × 𝑋𝑘×𝑛 matrix multiplication requires 2𝑚 × 𝑘 × 𝑛 FLOPs (factor of 2 needed to account for multiplies and adds).

For the attention block, the main FLOP contributors are the key, query, and value transformation (``6Bsh^2`` operations), 
attention matrix computation (``2Bs^2h`` operations), attention over values (``2Bs^2h`` operations), 
and post-attention linear projection (``2Bsh^2`` operations). The feed-forward network increases the hidden size to ``4h`` and then reduces it back to ``h``; this requires ``16Bsh^2`` FLOPs. 
Summing these together, each transformer layer results in ``24Bsh^2 + 4Bs^2h`` FLOPs for the forward pass.

The other main contributor to the FLOP count is the logit layer in the language model head, the required FLOPs for this operation is ``2Bsh𝑉`` in the forward pass and ``4Bsh𝑉`` in the backward pass, resulting in ``6Bsh𝑉`` FLOPs in total.

The backward pass requires double the number of FLOPs since need to calculate the gradients with respect to both input and weight tensors.

Thus, for a transformer model with ``l`` transformer layers, the total number of floating-point operations is:
```
C = C_forward + C_backward ≈ 2PD + 4PD ≈ 6PD
```
with parameter size ``P`` and the training dataset size ``D`` (tokens).

#### Parameters size
The number of parameters in a model ``P`` can be computed as:
```
P = 12lh^2 * [1 + 13/12h + (V + s)/(12lh)]
```
where number of layers ``l``, hidden size ``h``, vocabulary size ``V``, and sequence length ``s``.


### Inference CO2e calculation
The total carbon footprint calculation of inference is similar to training. Inference involves running the input data through the model's forward pass without performing 
any backward pass or gradient updates, thus the computation ``C_inference`` is approximated as
```
C_inference ≈ 2P * D_inference
```
where ``D_inference`` means inference dataset size (tokens).


## Using Impact Framework for estimation

### What is Impact Framework

[Impact Framework (IF)](https://if.greensoftware.foundation/) is an Open Source tool being run inside the Green Software Foundation designed to assess the environmental impact of software across various components and settings, aiming to minimize the ecological footprint of software. To utilize IF, you simply need to create a manifest file, after which the IF takes care of the remaining processes. This manifest file provides essential context for calculating the environmental impact, outlining the application's architecture, the duration of observation, the sequence of calculations and transformations to be performed, and the specific environmental metrics to be monitored.

Here is the video explain how IF works, it can help you better understand the capabilities of IF.

[![impact framework explainer video)](https://github.com/Green-Software-Foundation/hack/assets/11027021/541b9e21-85ab-47ab-8d43-c8e2111408a7)](https://www.youtube.com/watch?v=msk-55owTeM)

With the methodology outlined above for estimating LLM carbon emissions information, we can utilize the Impact Framework to assess the carbon footprint of the LLM. The Impact Framework offers a versatile and expandable framework for evaluating the carbon footprint of diverse computing activities, leveraging a variety of plugins to build upon the manifest.

### Basic Manifest for LLM Carbon Emissions

Based on the basic the total carbon footprint equation `CO2eq = CO2eq_oper + CO2eq_emb`, we can divide the total carbon footprint into two components: `CO2eq_oper`,the operational footprint, and `CO2eq_emb`, the embodied footprint.

#### Basic Operational Footprint Equation and Variables
The fundamental equation for `CO2eq_oper` is `CO2eq_oper = energy_oper * carb_inten`, where `energy_oper` represents the energy utilized during the operation of the LLM, and `carb_inten` denotes the carbon intensity of the energy consumed. To derive `energy_oper`, the [Watt-hour formula](https://arxiv.org/abs/2111.00364) `energy_oper(Wh) = GPU-num×GPU-h×TDP×PUE` is employed. Hence, acquiring `energy_oper` necessitates knowledge of the total hours for training an LLM (`GPU-num×GPU-h`), the power consumption of the GPU (TDP), and the Power Usage Effectiveness (`PUE`).

The equivalent training carbon footprint depends on:
- Total Training Time
- Number of GPUs
- Thermal Design Power(TDP) of GPUs
- Power Usage Effectiveness(PUE)
- Regional carbon equivalent emissions

The final equation for operational footprint is:

```
CO2eq_oper =  GPU-num*GPU-h*(GPU power consumption per GPU) * PUE * carb_inten
```

#### Basic Embodied Footprint Equation and Variables

#### Dive in Manifest

### Extended Manifest for LLM Carbon Emissions

## Conclusion

In this article, we explore the swift development of Large Language Models (LLMs) and the ongoing discussions surrounding their environmental impact. We detail the process of estimating carbon emissions during the training and inference stages of LLMs, utilizing the Impact framework tool to present a range of manifest examples that offer varying degrees of detailed emission estimates. These methods, alongside the manifest's input variables, enable a comparative analysis of the carbon footprint associated with different LLM configurations, encouraging efforts to minimize emissions. Additionally, we have assembled a comprehensive dataset encompassing the carbon emissions of current LLMs and pertinent public data for emission calculations, designed to streamline the process for users. By accurately quantifying the carbon emissions of LLMs and enhancing our understanding of their energy consumption, we are optimistic that a sustainable future, where AI advancements and environmental conservation are interwoven, is within reach.

## Reference
Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V.A., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., Phanishayee, A., & Zaharia, M.A. (2021). Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM. SC21: International Conference for High Performance Computing, Networking, Storage and Analysis, 1-14.

Kaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). Scaling Laws for Neural Language Models. ArXiv, abs/2001.08361.

Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Glo- ria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. 2022. Sustainable ai: Environmental implica- tions, challenges and opportunities. Proceedings of Machine Learning and Systems, 4:795–813.

Faiz, A., Kaneda, S., Wang, R., Osi, R., Sharma, P., Chen, F., & Jiang, L. (2023). LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models. ArXiv, abs/2309.14393.
