name: with estimated training time llm training manifest
description:
  "
  estimated traing time
  For an approximate training time calculation, we need to estimate the following:
  - Total train FLOPs required by the model
  - Benchmark of single GPU FLOPs
  - Percent of peak device throughput as estimated using the regression equation

  This gives the train time as: $t_{train} = \frac{\text{Total Train FLOPs}}{\text{(Benchmark FLOPs per GPU)}*\text{Percent Utilization}*\text{#GPUs}}$

  The total train compute can be defined as: $train_{compute} = T.P.f_p$ where <br>
  - $T$: Total Training Tokens
  - $P$: Total Parameters
  - $f_p$: FLOPs required per token per parameter

  training-operation-carbon: Wh = GPU-h×(GPU power consumption)×PUE and tCO2eq = MWh × Impact
  training-embodied-carbon: training days / hardware lifetime * (hardware num × hardware CPA)
  "
tags:
initialize:
  plugins:
    training-parameters-actived-multiply:
      method: Multiply
      path: '@grnsft/if-plugins'
      global-config:
        input-parameters: ['modal/parameters-count', 'modal/activated-rate-per-token']
        output-parameter: 'modal/parameters-actived-count'
    training-FLOP-count-multiply:
      method: Multiply
      path: '@grnsft/if-plugins'
      global-config:
        input-parameters: ['training-FLOP-count-factor', 'modal/parameters-actived-count', 'modal/tokens-count']
        output-parameter: 'training-FLOP-count'
    # training-gpu-meomory-capability-multiply:
    #   method: Multiply
    #   path: '@grnsft/if-plugins'
    #   global-config:
    #     input-parameters: ['gpu/memory', 'gpu/memory-capacity-rate']
    #     output-parameter: 'gpu/memory-capability'
    estimate-training-total-compute-multiply:
      method: Multiply
      path: '@grnsft/if-plugins'
      global-config:
        input-parameters: ['modal/flop-per-parameter', 'modal/parameters-count', 'modal/tokens-count']
        output-parameter: 'estimate-training-total-compute'
    estimate-training-compute-per-second-multiply:
      method: Multiply
      path: '@grnsft/if-plugins'
      global-config:
        input-parameters: ['resources-reserved', 'modal/estimated-throughput']
        output-parameter: 'estimate-training-compute-per-second'    
    estimate-training-time-divide:
      method: Divide
      path: '@grnsft/if-plugins'
      global-config:
        numerator: 'estimate-training-total-compute'
        denominator: 'estimate-training-compute-per-second'
        output: 'estimate-training-time-second'
    estimate-training-time-hour-divide:
      method: Divide
      path: '@grnsft/if-plugins'
      global-config:
        numerator: 'estimate-training-time-second'
        denominator: 'seconds-per-hour'
        output: 'estimate-training-time-hour'    
    training-operation-carbon-multiply:
      method: Multiply
      path: '@grnsft/if-plugins'
      global-config:
        input-parameters: ['resources-reserved', 'estimate-training-time-hour', 'gpu/powerConsumption', 'pue', 'carbonImpact']
        output-parameter: 'operation-carbon'
    device-expected-lifespan-hours-per-year-multiply:
      method: Multiply
      path: '@grnsft/if-plugins'
      global-config:
        input-parameters: ['device/expected-lifespan', 'days-per-year', 'hours-per-day']
        output-parameter: 'expected-lifespan-duration'
    reserved-device-hour-with-device-expected-lifespan-divide:
      method: Divide
      path: '@grnsft/if-plugins'
      global-config:
        numerator: 'estimate-training-time-hour'
        denominator: 'expected-lifespan-duration'
        output: 'expected-lifespan-rate'
    embodied-carbon-multiply:
      method: Multiply
      path: '@grnsft/if-plugins'
      global-config:
        input-parameters: ['resources-reserved', 'expected-lifespan-rate','device/emissions-embodied', 'thousands-per-unit']
        output-parameter: 'carbon-embodied'
    sum:
      method: Sum
      path: '@grnsft/if-plugins'
      global-config:
        input-parameters: [ 'operation-carbon', 'carbon-embodied' ]
        output-parameter: 'total-carbon'
tree:
  children:
    child:
      pipeline:
        - training-operation-carbon-multiply
        - device-expected-lifespan-hours-per-year-multiply
        - reserved-device-hour-with-device-expected-lifespan-divide
        - embodied-carbon-multiply
        - sum
      defaults:
        training-FLOP-count-factor: 6
        thousands-per-unit: 0.001
        days-per-year: 365
        hours-per-day: 24
        seconds-per-hour: 3600
        node-size: 8 # the GPU node size, 8 is recommended
        # gpu/memory-capacity-rate: 0.03 # we eastimate that a 80GB GPU can store a 2.4B parameter model, which measns a GPU with a memory capacity of approximately 166.67 GB would be required to store a model with 5 billion parameters.
        device/emissions-embodied: 1533.120 # in gCO2e for total resource units
        device/expected-lifespan: 5 # 5 years in seconds.
        # Meta’s data centers achieve an average utilization rate of 60% throughout the 5-year lifespan of hardware units
        # ref: Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable ai: Environmental impli- cations, challenges and opportunities. Proceedings of Machine Learning and Systems, 4:795–813, 2022.
        resources-reserved: 1 # resource units reserved / used
        #  estimated total number of GPUs: $n$ Use regression coefficients for estimating number of GPUs $n$ 
        # num_gpu = np.round(func_gpu(P)/model_size)*model_size
      inputs:
        - timestamp: 2023-08-06T00:00
          gpu/memory: 80 # GB
          gpu/powerConsumption: 400 # kWh use tdp
          gpu/duration: 82432 # hour
          modal/parameters-count: 1000000000 # billions parameters
          modal/tokens-count: 3000000000000 # trillions tokens
          modal/activated-rate-per-token: 1 # defalut 100% actived
          modal/estimated-throughput: 1 #  if (t_size <= node_size and p_size == 1): intra model condition X = func_tensor(P)； else: inter model X = func_pipe(P)
          # efficiency
          modal/flop-per-parameter: 6 
          hardware/CPA: 1533.120
          pue: 1.1
          carbonImpact: 0.385 # the US national average carbon intensity factor of 0.385 kg CO2eq/KWh
